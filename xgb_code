# this code was used to generate XGBoost models, with relative abundances of bacterial genera as independent variables (X)
# and individual metabolites ad the dependent variables (y). A separated model was generated for each metabolite. 

# define X and y 
X= micro.copy()
X.columns= X.columns.str.replace(r"[\[\]<>]", "", regex=True)

# define param_grid for grid search
param_grid= {
     'n_estimators': [50, 100, 200],
    'max_depth': [3, 5, 7],
    'learning_rate': [0.01, 0.1, 0.3],
    'colsample_bytree': [0.8, 1]
}

# define cross validation 
cv= KFold(n_splits=10, shuffle=True, random_state=42)
scorer= make_scorer(mean_squared_error, greater_is_better= False)

# lista para almacenar resultados 
resultados= [] 

# loop for para iterar en cada metabolito del df mtb2 (normalizados)
for metabolito in mtb2.columns: 
    y= mtb2[metabolito]
    
    # definimos otra vez el modelo xgb que se generar√° en cada iteracion del loop
    xgb= XGBRegressor(objective='reg:squarederror', random_state=42)
    
    # definimos el Grid Search 
    grid_search= GridSearchCV(estimator= xgb,
                              param_grid=param_grid,
                              scoring=scorer, 
                              cv=cv,
                              verbose=0,
                              n_jobs=-1)
    grid_search.fit(X,y)

    # hacemos predicciones 
    y_pred= grid_search.best_estimator_.predict(X)
    # evaluamos el modelo 
    mse= mean_squared_error(y, y_pred)
    r2= r2_score(y, y_pred)

    # almacenamos resultados para poder crear df 
    resultados.append({
        'metabolito': metabolito,
        'mejores_hiperparametros': grid_search.best_params_,
        'MSE': mse,
        'R2': r2
    })
# converimos a data frame
df_resultados= pd.DataFrame(resultados)
